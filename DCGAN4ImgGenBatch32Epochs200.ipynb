{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here\u0027s several helpful packages to load in \n",
        "\n",
        "# Topic: Adversarial Training of Neural Networks\n",
        "# 0. Introduction (1 page) !Not Important for now\n",
        "# 1. Problem statement (3-4 pages) \n",
        "# 2. Theoritical background (5-6 pages)\n",
        "# 3. The technology stack and architecture (3 pages)\n",
        "# 4. Description of solution (2-3 pages)\n",
        "# 5. Effects of Hyperparameter Tuniung (3 pages)\n",
        "# 6. Observations (3 pages)\n",
        "# 7. Conclusion (1 page)\n",
        "# 8. References (1 page)\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "def generator(z, output_channel_dim, training):\n    with tf.variable_scope(\"generator\", reuse\u003d not training):\n    \n        \n        # 8x8x1024\n        fully_connected \u003d tf.layers.dense(z, 8*8*1024)\n        fully_connected \u003d tf.reshape(fully_connected, (-1, 8, 8, 1024))\n        fully_connected \u003d tf.nn.leaky_relu(fully_connected)\n\n        # 8x8x1024 -\u003e 16x16x512\n        trans_conv1 \u003d tf.layers.conv2d_transpose(inputs\u003dfully_connected,\n                                                 filters\u003d512,\n                                                 kernel_size\u003d[5,5],\n                                                 strides\u003d[2,2],\n                                                 padding\u003d\"SAME\",\n                                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n                                                 name\u003d\"trans_conv1\")\n        batch_trans_conv1 \u003d tf.layers.batch_normalization(inputs \u003d trans_conv1,\n                                                          training\u003dtraining,\n                                                          epsilon\u003dEPSILON,\n                                                          name\u003d\"batch_trans_conv1\")\n        trans_conv1_out \u003d tf.nn.leaky_relu(batch_trans_conv1,\n                                           name\u003d\"trans_conv1_out\")\n        \n        # 16x16x512 -\u003e 32x32x256\n        trans_conv2 \u003d tf.layers.conv2d_transpose(inputs\u003dtrans_conv1_out,\n                                                 filters\u003d256,\n                                                 kernel_size\u003d[5,5],\n                                                 strides\u003d[2,2],\n                                                 padding\u003d\"SAME\",\n                                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n                                                 name\u003d\"trans_conv2\")\n        batch_trans_conv2 \u003d tf.layers.batch_normalization(inputs \u003d trans_conv2,\n                                                          training\u003dtraining,\n                                                          epsilon\u003dEPSILON,\n                                                          name\u003d\"batch_trans_conv2\")\n        trans_conv2_out \u003d tf.nn.leaky_relu(batch_trans_conv2,\n                                           name\u003d\"trans_conv2_out\")\n        \n        # 32x32x256 -\u003e 64x64x128\n        trans_conv3 \u003d tf.layers.conv2d_transpose(inputs\u003dtrans_conv2_out,\n                                                 filters\u003d128,\n                                                 kernel_size\u003d[5,5],\n                                                 strides\u003d[2,2],\n                                                 padding\u003d\"SAME\",\n                                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n                                                 name\u003d\"trans_conv3\")\n        batch_trans_conv3 \u003d tf.layers.batch_normalization(inputs \u003d trans_conv3,\n                                                          training\u003dtraining,\n                                                          epsilon\u003dEPSILON,\n                                                          name\u003d\"batch_trans_conv3\")\n        trans_conv3_out \u003d tf.nn.leaky_relu(batch_trans_conv3,\n                                           name\u003d\"trans_conv3_out\")\n        \n        # 64x64x128 -\u003e 128x128x64\n        trans_conv4 \u003d tf.layers.conv2d_transpose(inputs\u003dtrans_conv3_out,\n                                                 filters\u003d64,\n                                                 kernel_size\u003d[5,5],\n                                                 strides\u003d[2,2],\n                                                 padding\u003d\"SAME\",\n                                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n                                                 name\u003d\"trans_conv4\")\n        batch_trans_conv4 \u003d tf.layers.batch_normalization(inputs \u003d trans_conv4,\n                                                          training\u003dtraining,\n                                                          epsilon\u003dEPSILON,\n                                                          name\u003d\"batch_trans_conv4\")\n        trans_conv4_out \u003d tf.nn.leaky_relu(batch_trans_conv4,\n                                           name\u003d\"trans_conv4_out\")\n        \n        # 128x128x64 -\u003e 128x128x3\n        logits \u003d tf.layers.conv2d_transpose(inputs\u003dtrans_conv4_out,\n                                            filters\u003d3,\n                                            kernel_size\u003d[5,5],\n                                            strides\u003d[1,1],\n                                            padding\u003d\"SAME\",\n                                            kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n                                            name\u003d\"logits\")\n        out \u003d tf.tanh(logits, name\u003d\"out\")\n        return out"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def discriminator(x, reuse):\n",
        "    with tf.variable_scope(\"discriminator\", reuse\u003dreuse): \n",
        "        \n",
        "        # 128*128*3 -\u003e 64x64x64 \n",
        "        conv1 \u003d tf.layers.conv2d(inputs\u003dx,\n",
        "                                 filters\u003d64,\n",
        "                                 kernel_size\u003d[5,5],\n",
        "                                 strides\u003d[2,2],\n",
        "                                 padding\u003d\"SAME\",\n",
        "                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n",
        "                                 name\u003d\u0027conv1\u0027)\n",
        "        batch_norm1 \u003d tf.layers.batch_normalization(conv1,\n",
        "                                                    training\u003dTrue,\n",
        "                                                    epsilon\u003dEPSILON,\n",
        "                                                    name\u003d\u0027batch_norm1\u0027)\n",
        "        conv1_out \u003d tf.nn.leaky_relu(batch_norm1,\n",
        "                                     name\u003d\"conv1_out\")\n",
        "        \n",
        "        # 64x64x64-\u003e 32x32x128 \n",
        "        conv2 \u003d tf.layers.conv2d(inputs\u003dconv1_out,\n",
        "                                 filters\u003d128,\n",
        "                                 kernel_size\u003d[5, 5],\n",
        "                                 strides\u003d[2, 2],\n",
        "                                 padding\u003d\"SAME\",\n",
        "                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n",
        "                                 name\u003d\u0027conv2\u0027)\n",
        "        batch_norm2 \u003d tf.layers.batch_normalization(conv2,\n",
        "                                                    training\u003dTrue,\n",
        "                                                    epsilon\u003dEPSILON,\n",
        "                                                    name\u003d\u0027batch_norm2\u0027)\n",
        "        conv2_out \u003d tf.nn.leaky_relu(batch_norm2,\n",
        "                                     name\u003d\"conv2_out\")\n",
        "        \n",
        "        # 32x32x128 -\u003e 16x16x256  \n",
        "        conv3 \u003d tf.layers.conv2d(inputs\u003dconv2_out,\n",
        "                                 filters\u003d256,\n",
        "                                 kernel_size\u003d[5, 5],\n",
        "                                 strides\u003d[2, 2],\n",
        "                                 padding\u003d\"SAME\",\n",
        "                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n",
        "                                 name\u003d\u0027conv3\u0027)\n",
        "        batch_norm3 \u003d tf.layers.batch_normalization(conv3,\n",
        "                                                    training\u003dTrue,\n",
        "                                                    epsilon\u003dEPSILON,\n",
        "                                                    name\u003d\u0027batch_norm3\u0027)\n",
        "        conv3_out \u003d tf.nn.leaky_relu(batch_norm3,\n",
        "                                     name\u003d\"conv3_out\")\n",
        "        \n",
        "        # 16x16x256 -\u003e 16x16x512\n",
        "        conv4 \u003d tf.layers.conv2d(inputs\u003dconv3_out,\n",
        "                                 filters\u003d512,\n",
        "                                 kernel_size\u003d[5, 5],\n",
        "                                 strides\u003d[1, 1],\n",
        "                                 padding\u003d\"SAME\",\n",
        "                                 kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n",
        "                                 name\u003d\u0027conv4\u0027)\n",
        "        batch_norm4 \u003d tf.layers.batch_normalization(conv4,\n",
        "                                                    training\u003dTrue,\n",
        "                                                    epsilon\u003dEPSILON,\n",
        "                                                    name\u003d\u0027batch_norm4\u0027)\n",
        "        conv4_out \u003d tf.nn.leaky_relu(batch_norm4,\n",
        "                                     name\u003d\"conv4_out\")\n",
        "        \n",
        "        # 16x16x512 -\u003e 8x8x1024\n",
        "        conv5 \u003d tf.layers.conv2d(inputs\u003dconv4_out,\n",
        "                                filters\u003d1024,\n",
        "                                kernel_size\u003d[5, 5],\n",
        "                                strides\u003d[2, 2],\n",
        "                                padding\u003d\"SAME\",\n",
        "                                kernel_initializer\u003dtf.truncated_normal_initializer(stddev\u003dWEIGHT_INIT_STDDEV),\n",
        "                                name\u003d\u0027conv5\u0027)\n",
        "        batch_norm5 \u003d tf.layers.batch_normalization(conv5,\n",
        "                                                    training\u003dTrue,\n",
        "                                                    epsilon\u003dEPSILON,\n",
        "                                                    name\u003d\u0027batch_norm5\u0027)\n",
        "        conv5_out \u003d tf.nn.leaky_relu(batch_norm5,\n",
        "                                     name\u003d\"conv5_out\")\n",
        "\n",
        "        flatten \u003d tf.reshape(conv5_out, (-1, 8*8*1024))\n",
        "        logits \u003d tf.layers.dense(inputs\u003dflatten,\n",
        "                                 units\u003d1,\n",
        "                                 activation\u003dNone)\n",
        "        out \u003d tf.sigmoid(logits)\n",
        "        return out, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def model_loss(input_real, input_z, output_channel_dim):\n",
        "    g_model \u003d generator(input_z, output_channel_dim, True)\n",
        "\n",
        "    noisy_input_real \u003d input_real + tf.random_normal(shape\u003dtf.shape(input_real),\n",
        "                                                     mean\u003d0.0,\n",
        "                                                     stddev\u003drandom.uniform(0.0, 0.1),\n",
        "                                                     dtype\u003dtf.float32)\n",
        "    \n",
        "    d_model_real, d_logits_real \u003d discriminator(noisy_input_real, reuse\u003dFalse)\n",
        "    d_model_fake, d_logits_fake \u003d discriminator(g_model, reuse\u003dTrue)\n",
        "    \n",
        "    d_loss_real \u003d tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits\u003dd_logits_real,\n",
        "                                                                         labels\u003dtf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\n",
        "    d_loss_fake \u003d tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits\u003dd_logits_fake,\n",
        "                                                                         labels\u003dtf.zeros_like(d_model_fake)))\n",
        "    d_loss \u003d tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\n",
        "    g_loss \u003d tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits\u003dd_logits_fake,\n",
        "                                                                    labels\u003dtf.ones_like(d_model_fake)))\n",
        "    return d_loss, g_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def model_optimizers(d_loss, g_loss):\n",
        "    t_vars \u003d tf.trainable_variables()\n",
        "    g_vars \u003d [var for var in t_vars if var.name.startswith(\"generator\")]\n",
        "    d_vars \u003d [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
        "    \n",
        "    update_ops \u003d tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    gen_updates \u003d [op for op in update_ops if op.name.startswith(\u0027generator\u0027)]\n",
        "    \n",
        "    with tf.control_dependencies(gen_updates):\n",
        "        d_train_opt \u003d tf.train.AdamOptimizer(learning_rate\u003dLR_D, beta1\u003dBETA1).minimize(d_loss, var_list\u003dd_vars)\n",
        "        g_train_opt \u003d tf.train.AdamOptimizer(learning_rate\u003dLR_G, beta1\u003dBETA1).minimize(g_loss, var_list\u003dg_vars)  \n",
        "    return d_train_opt, g_train_opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def model_inputs(real_dim, z_dim):\n",
        "    inputs_real \u003d tf.placeholder(tf.float32, (None, *real_dim), name\u003d\u0027inputs_real\u0027)\n",
        "    inputs_z \u003d tf.placeholder(tf.float32, (None, z_dim), name\u003d\"input_z\")\n",
        "    learning_rate_G \u003d tf.placeholder(tf.float32, name\u003d\"lr_g\")\n",
        "    learning_rate_D \u003d tf.placeholder(tf.float32, name\u003d\"lr_d\")\n",
        "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def show_samples(sample_images, name, epoch):\n",
        "    figure, axes \u003d plt.subplots(1, len(sample_images), figsize \u003d (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    curr_dir_to_write \u003d output_dir + \"/\" + str(epoch) + \"/\"\n",
        "    if not os.path.exists(curr_dir_to_write):\n",
        "        os.makedirs(curr_dir_to_write)\n",
        "    for index, axis in enumerate(axes):\n",
        "        axis.axis(\u0027off\u0027)\n",
        "        image_array \u003d sample_images[index]\n",
        "        axis.imshow(image_array)\n",
        "        image \u003d Image.fromarray(image_array)\n",
        "        image.save(curr_dir_to_write+str(epoch)+\"_\"+str(index)+\".png\")       \n",
        "    plt.savefig(curr_dir_to_write+str(epoch)+\".png\", bbox_inches\u003d\u0027tight\u0027, pad_inches\u003d0)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def test(sess, input_z, out_channel_dim, epoch):\n",
        "    curr_dir_to_write \u003d output_dir + \"/\" + str(epoch) + \"/\"\n",
        "    if not os.path.exists(curr_dir_to_write):\n",
        "        os.makedirs(curr_dir_to_write)\n",
        "    example_z \u003d np.random.uniform(-1, 1, size\u003d[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\n",
        "    samples \u003d sess.run(generator(input_z, out_channel_dim, False), feed_dict\u003d{input_z: example_z})\n",
        "    sample_images \u003d [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
        "    show_samples(sample_images, output_dir + \"samples\", epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def get_batches(data):\n",
        "    batches \u003d []\n",
        "    for i in range(int(data.shape[0]//BATCH_SIZE)):\n",
        "        batch \u003d data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        augmented_images \u003d []\n",
        "        for img in batch:\n",
        "            image \u003d Image.fromarray(img)\n",
        "            if random.choice([True, False]):\n",
        "                image \u003d image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            augmented_images.append(np.asarray(image))\n",
        "        batch \u003d np.asarray(augmented_images)\n",
        "        normalized_batch \u003d (batch / 127.5) - 1.0\n",
        "        batches.append(normalized_batch)\n",
        "    return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def train(get_batches, data_shape, checkpoint_to_load\u003dNone):\n",
        "    input_images, input_z, lr_G, lr_D \u003d model_inputs(data_shape[1:], NOISE_SIZE)\n",
        "    d_loss, g_loss \u003d model_loss(input_images, input_z, data_shape[3])\n",
        "    d_opt, g_opt \u003d model_optimizers(d_loss, g_loss)\n",
        "    \n",
        "    saver \u003d tf.train.Saver()\n",
        "    global SAVE_OR_RESTORE\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        if SAVE_OR_RESTORE:\n",
        "                saver.restore(sess, \"./model/model.ckpt\")\n",
        "                SAVE_OR_RESTORE \u003d False\n",
        "\n",
        "        epoch \u003d 0\n",
        "        iteration \u003d 0\n",
        "        d_losses \u003d []\n",
        "        g_losses \u003d []\n",
        "        \n",
        "        for epoch in range(EPOCHS):        \n",
        "            epoch +\u003d 1\n",
        "            start_time \u003d time.time()\n",
        "            \n",
        "            for batch_images in get_batches:\n",
        "                iteration +\u003d 1\n",
        "                batch_z \u003d np.random.uniform(-1, 1, size\u003d(BATCH_SIZE, NOISE_SIZE))\n",
        "                _ \u003d sess.run(d_opt, feed_dict\u003d{input_images: batch_images, input_z: batch_z, lr_D: LR_D})\n",
        "                _ \u003d sess.run(g_opt, feed_dict\u003d{input_images: batch_images, input_z: batch_z, lr_G: LR_G})\n",
        "                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\n",
        "                g_losses.append(g_loss.eval({input_z: batch_z}))\n",
        "\n",
        "            saver.save(sess, \"./model/model.ckpt\")\n",
        "            print(epoch)\n",
        "            summarize_epoch(epoch, time.time()-start_time, sess, d_losses, g_losses, input_z, data_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def summarize_epoch(epoch, duration, sess, d_losses, g_losses, input_z, data_shape):\n",
        "    minibatch_size \u003d int(data_shape[0]//BATCH_SIZE)\n",
        "    curr_dir_to_write \u003d output_dir + \"/\" + str(epoch) + \"/\"\n",
        "    if not os.path.exists(curr_dir_to_write):\n",
        "        os.makedirs(curr_dir_to_write)\n",
        "    print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n",
        "          \"\\nDuration: {:.5f}\".format(duration),\n",
        "          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n",
        "          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])), \n",
        "          file\u003dopen(curr_dir_to_write + \"stats\" + str(epoch) + \".txt\", \"a+\"))\n",
        "    fig, ax \u003d plt.subplots()\n",
        "    plt.plot(d_losses, label\u003d\u0027Discriminator\u0027, alpha\u003d0.6)\n",
        "    plt.plot(g_losses, label\u003d\u0027Generator\u0027, alpha\u003d0.6)\n",
        "    plt.title(\"Losses\")\n",
        "    plt.legend()\n",
        "    plt.savefig(curr_dir_to_write + \"losses_\" + str(epoch) + \".png\")\n",
        "    plt.close()\n",
        "    test(sess, input_z, data_shape[3], epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "IMAGE_SIZE \u003d 128\n",
        "SAVE_OR_RESTORE \u003d True\n",
        "NOISE_SIZE \u003d 100\n",
        "LR_D \u003d 0.00004\n",
        "LR_G \u003d 0.0004\n",
        "BATCH_SIZE \u003d 32\n",
        "EPOCHS \u003d 200\n",
        "BETA1 \u003d 0.5\n",
        "WEIGHT_INIT_STDDEV \u003d 0.02\n",
        "EPSILON \u003d 0.00005\n",
        "SAMPLES_TO_SHOW \u003d 10\n",
        "LOWER_LIMIT \u003d 15000\n",
        "UPPER_LIMIT \u003d 17000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "path_to_files \u003d \"/floyd/input/flickr_faces_thumbnails_nvlabs_dataset/\"\n",
        "    \n",
        "output_dir \u003d \u0027./{date:%Y-%m-%d_%H:%M:%S}/\u0027.format(date\u003ddatetime.datetime.now())\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n"
          ]
        }
      ],
      "source": [
        "numpy_image_array \u003d np.asarray([np.asarray(Image.open(file)) for file in glob(path_to_files + \"*.png\")])[LOWER_LIMIT:UPPER_LIMIT]\n",
        "np.random.shuffle(numpy_image_array)\n",
        "\n",
        "sample_images \u003d random.sample(list(numpy_image_array), SAMPLES_TO_SHOW)\n",
        "show_samples(sample_images, output_dir + \"inputs\", 0)\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    train(get_batches(numpy_image_array), numpy_image_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}